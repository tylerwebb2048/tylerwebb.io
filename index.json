[{"content":"In his book, Tools and Weapons, Brad Smith lays out a series of important considerations for the continued adoption of artificial intelligence into an ever increasing amount of applications and daily uses. While artificial intelligence has historically held a rather loose definition, Brad Smith quotes Dave Heiner of Microsoft who describes AI as \u0026ldquo;a computer system that can learn from experience by discerning patterns in data fed to it and thereby make decisions\u0026rdquo; (Tools and Weapons pg. 194). This ability to learn based on experience and modeling is integral to creating flexible systems that can react to their environments in meaningful ways.\nAI has recently been emboldened by a number of technological advances. This includes massive advances in the scalability and accessibility of computing power. Commonly this exists in the form of a datacenter as part of the cloud which doesn\u0026rsquo;t require large, initial capital investments. This increase in centralized data storage and computing power has also provided for a deep array of data from which to train models. These advances have come together to allow the pace of development for artificial intelligence to increase rapidly.\nBrad Smith covers the advances and his proposed guidelines for AI across 3 primary chapters. I\u0026rsquo;ve written some of my general thoughts on each below.\nAI and Ethics: Don\u0026rsquo;t Ask What Computers Can Do, Ask What They Should Do If you\u0026rsquo;re driving down any mountain road in the United States or many places in the world, you’re likely to see a metal railing running along potential drop offs and dangerous areas. In an ideal situation these guardrails should never come in contact with your car as it moves and steers down the mountain. However, in very rare cases, a car can experience a mechanical problem or a driver can lose focus. When this happens the guardrails are there to guide the car away from a cliff or hazard and give the driver time to get the car under control, slow down, and stop to assess.\nAs we continue to ramp up the speed of the innovation and development of artificial intelligence, it will be equally important to develop guardrails to keep us on track. Technology can be used for a variety of uses but it needs to be pointed in a direction and given a use. Oftentimes, we focus on the rapid development of a new technology because it is possible and not always based on its appropriate or ethical use. Purposeful development of artificial intelligence, with the appropriate guardrails in place, will allow us to continue to fill needs and innovate while protecting those things that are important to the success of humanity and individuals.\nMicrosoft has put together 4 basic principles for AI development in order to avoid potential pitfalls. These are:\nFairness Reliability and Safety Privacy and Security User Inclusiveness These 4 principles help to provide a basis for development while attempting to respect the human rights across all groups. While simple in list form, each of these principles are deep in their application requirements and considerations. Further considerations will also need to be made on additional guiding principles and development considerations. Brad describes Microsoft\u0026rsquo;s past experience with determining these factors:\n\u0026ldquo;As computers gained the ability to make decisions previously reserved for humans, virtually every ethical question for humanity was becoming an ethical question for computing. If millennia of debate among philosophers had not forged clear cut and universal answers, then a consensus was not likely to emerge overnight simply because we needed to apply them to computers.\u0026rdquo; - Tools and Weapons pg. 199\nIn order to further determine what to use as guardrails and driving principles for the creation of artificial intelligence, humanity will continue to need to determine and address what it collectively holds most dear. Only then can we move forward with confidence into a world filled with automated decision making.\nAI and Facial Recognition: Do Our Faces Deserve the Same Protection as Our Phones? The human body is an incredible creation. It is universally identifiable, yet individually unique. With many unique, yet similar features, there exists the capacity for humans and machines to identify at the level of an individual. The face is an important identifying characteristic and serves as the viewport to the world for a multitude of senses.\nMany innovations have already taken advantage of the unique nature of the human face. One of the most widely spread uses of this so far has been in the world of biometric digital security. It is now possible on many devices to simply smile (or not) at the camera and unlock your digital rights of access. Where this has provided a new level of convenience, it has also raised interesting questions and concerns around the ownership of these new data sets and who has the justification to employ them.\nOther innovations have been employed to recognize the face as part of scalable applications related to everything from automatic photo tagging and sharing to the identification of individuals as part of investigations. There are some key components to the questions of not just legality but the ethics behind these and other applications of facial recognition technology. When the ability to stay a private citizen is lost it will be incredibly hard to attain again.\nAI and the Workforce: The Day the Horse Lost Its Job Throughout time, humanity has worked to automate the mundane to enable it to focus on greater pursuits. This has almost always involved the strategic application of technology and processes. This was seen in the early days of humanity, when the transition from hunting and gathering to agricultural production was underway. It has been seen in more recent times with the replacement of horses for cars, and in the modern day as the communication of information has evolved and changed to focus on accessibility of access and creation. This period of transition that occurs time and time again has usually ended in the benefit of the human race, however these transitions are not without negative impact, sometimes even drastically so.\nArtificial Intelligence has a unique potential to generate not just one, but many transitions for the workforce. It will affect many, if not all, lines of work in some way. Some current jobs stand to be outright replaced, where many are likely to be modified with a focus on strategic decision making and communication. Brad points out that pay scale does not have anything definitively to do with the risk of replacement or modification. While lower skilled jobs such as the order attendant or cashier at a store are potentially at risk, so are higher paying jobs like semi-truck drivers and radiologists. AI has the unique potential to affect jobs and life at all levels and should be given careful consideration for each potential application to ease the transition.\nThere will continue to be a need to provide better educational opportunities and clarity of job positions in order to adequately support humanity through the transitions to come. When looking at what these transitions could potentially mean for Microsoft, Brad Smith recounted the following important principles:\n\u0026ldquo;As we thought about what this meant for our own products and future at Microsoft, we concluded that success has always required that people master four skills: learning about new topics and fields; analyzing and solving new problems; communicating ideas and sharing information with others; and collaborating effectively as part of a team.\u0026rdquo; - Tools and Weapons pg. 247\nThese areas, and others, will continue to be important to focus on to limit those left behind by change and lift up humanity together as Artificial Intelligence adoption continues to ramp up its prevalence.\nConclusion These questions are at the core of the continued rapid innovation of Artificial Intelligence and there will be many more. It will be critical to pay attention to not just the how but also the why of the application of these new technologies. Without it, many important freedoms and opportunities will run the risk of being eroded away without a likely potential of attaining them again. It is up to governments, companies, and individuals worldwide to consider the complex and diverse ramifications and to make sure that those who are potentially at risk are not taken advantage of or left in the proverbial dust. Only then can humanity confidently take steps forward into the exciting and terrible world of technology making decisions by and for itself.\nFind the Book Here: Tools and Weapons by Brad Smith\n","permalink":"https://tylerwebb.io/posts/tools_and_weapons_part_4_artificial_intelligence-copy/","summary":"\u003cp\u003eIn his book, Tools and Weapons, Brad Smith lays out a series of important considerations for the continued adoption of artificial intelligence into an ever increasing amount of applications and daily uses. While artificial intelligence has historically held a rather loose definition, Brad Smith quotes Dave Heiner of Microsoft who describes AI as \u0026ldquo;a computer system that can learn from experience by discerning patterns in data fed to it and thereby make decisions\u0026rdquo; (Tools and Weapons pg. 194). This ability to learn based on experience and modeling is integral to creating flexible systems that can react to their environments in meaningful ways.\u003c/p\u003e","title":"Tools and Weapons Review Part 4: How to Prepare for a World Empowered by Artifical Intelligence"},{"content":"Rural Broadband: The Electricity of the Twenty-First Century Electricity is truly one of humanity\u0026rsquo;s most impactful and profound discoveries. We\u0026rsquo;ve successfully harnessed electrons to rid our world of darkness at a moment\u0026rsquo;s notice, communicate with other people over large distances, drive our cars, restart human hearts, and so much more. The harnessing of electricity has been an innovation that has seen no bounds. It has wove itself into the very fabric of humanity and had drastic effects that one no could have possibly predicted.\nElectricity was not, and still continues to not, be an overnight adoption for all. Innovations in urban settings took decades to be adopted by rural populations. This uneven transition had to do with a complicated array of factors but a big proponent was return on investment. Where in urban environments, electrical companies could reach a large number of customers with a small run of cabling, in rural settings large amounts of capital investment had to be made to reach a small number of customers.\nA very similar transition has occurred with the adoption of the internet. Wherein many residents of cities have access to high speed network connections for a relatively low cost (due to economies of scale), the same rules do not necessarily apply to the installation of internet runs in rural communities. This has left rural populations at a huge disadvantage in terms of the empowering access of the internet. Many rural areas of the United States are left to struggle with slow speeds provided by existing phone lines or paying for limited and expensive satellite or LTE plans. The cost vs reward for traditional cable companies to run their services into rural communities has not been rewarding enough to rapidly deploy the core infrastructure needed.\nAs Brad Smith discusses, the internet, and especially high speed internet, has become the electricity conundrum of the modern age. Microsoft has taken huge steps in partnerships to begin deploying fixes for these communities including the deployment of coverage via TV white spaces technology to remove the need for expensive capital investments.\nAnother interesting strategy to bring broadband connection to not just rural areas in the United States, but potentially the entire world, is the deployment of satellite mesh networks by companies like SpaceX\u0026rsquo;s Starlink. These networks will in theory solve many of the problems of traditional satellite internet which has been tradionally provided via geostationary platforms.\nIn rating internet connections there are two key factors to consider. The first of these is commonly referred to as speed. This refers to the throughput which is how much data can be transferred at one time over a connection. The second consideration is the latency or response time of the network. A major problem with geostationary satellites is that they have to be very far away from Earth in order to lock into a stationary orbit. This requires more powerful communication devices to connect with these satellites and the communication delay (or latency) becomes very noticeable. This results in low throughput and slow responses which hampers many applications of the internet in the modern age. The cost of putting these satellites in orbit is also very high which ends up distributing the cost across the users and causes prices for these services to be high as well.\nSatellite mesh networks aim to solve all of these problems. The idea here is to launch a large number of satellites (thousands and potentially tens of thousands) into circular orbits. This allows the satellites to be deployed much closer to the Earth (1/8 the distance) and decreases not only latency but also the maximum throughput. As satellites move out of transmission range from the connection on the ground, the ground station (which will be similar to a cable modem) will be automatically routed to another satellite in range.\nThese satellite mesh networks have the potential to connect the entire world to an internet connection that has high throughput, low latency, and high redundancy (due to the scale of deployment).\nIn order to level the playing field in terms of access to the internet, these, and other, technology innovations will need to be put in place.\nAs Brad Smith puts it:\n\u0026ldquo;Ultimately, we need a national crusade to focus on and close the broadband gap. We need to recognize that, as was the case with electricity, a country separated by broadband availability will remain a nation more divided overall.\u0026rdquo; - Tools and Weapons pg. 163\nThe time is now to focus on bringing these rural communities, not just the United States but worldwide, the opportunities that come with full access to the information age. They have many valuable ideas and things to offer the rest of the world and it is important to provide them the same opportunities awarded to the rest of the world.\nLearn more about Microsoft\u0026rsquo;s Rural Broadband Program: https://www.microsoft.com/en-us/corporate-responsibility/airband/technology\n","permalink":"https://tylerwebb.io/posts/tools_and_weapons_part_3_connecting_the_world-copy/","summary":"\u003ch1 id=\"rural-broadband-the-electricity-of-the-twenty-first-century\"\u003eRural Broadband: The Electricity of the Twenty-First Century\u003c/h1\u003e\n\u003chr\u003e\n\u003cp\u003eElectricity is truly one of humanity\u0026rsquo;s most impactful and profound discoveries. We\u0026rsquo;ve successfully harnessed electrons to rid our world of darkness at a moment\u0026rsquo;s notice, communicate with other people over large distances, drive our cars, restart human hearts, and so much more. The harnessing of electricity has been an innovation that has seen no bounds. It has wove itself into the very fabric of humanity and had drastic effects that one no could have possibly predicted.\u003c/p\u003e","title":"Tools and Weapons Review Part 3: How to Level the Playing Field for all Internet Users"},{"content":"Cyber Security: The Wake-up Call for the World Humans love shortcuts. We love finding better, easier, and quicker ways of doing things. This has allowed us to innovate and automate away many of the mundane parts of life. It also has allowed us to progress the general standard of living exponentially.\nThis shortcut nature usually proves us well. We\u0026rsquo;re scrappy, we\u0026rsquo;re creative, and we know how to handle hard things. However, a critical ingredient to healthy progression is ethical direction. When Ethics are lacking, individuals and groups tend to look for shortcuts that bypass important issues such as human rights and ownership.\nThese principles are nothing new, but they\u0026rsquo;ve received a new life and medium in the digital world. There are many nefarious motivations to commit cyber crime. These can be for financial, vengeful, or political reasons, among many others. Whatever the reason, the side effects of cyber crime are negative and hurt individuals, organizations, and countries in various ways. This is why the security of digital systems has been, and will continue to be, incredibly important.\nBrad Smith takes us inside a number of high profile cyber attacks and the reactions and remediations that were taken for each. This includes the massive widespread attacks and damages that occurred from the WannaCry ransomware worm, a tool that was enabled with the weaponization of a vulnerability attributed to the National Security Agency and subsequently leaked by the Shadow Broker group. The final WannaCry package was potentially packaged and released by a group associated with the North Korean government. WannaCry proves the power of weaponized vulernabilities and is an example of a sophisticated Cyber Weapon.\nAs more of the world becomes automated and interconnected through a variety of networks, the potential to wreak havoc at scale and from a remote and anonymous position continues to grow as well. It will continue to become imperative for companies and governments across the earth to work together to control their cyber weapons and come to some necessary agreements. In a world where attribution is not as simple as just looking at satellite imagery to see who launched a missile or mobilized their forces, new levels of trust and defensive capabilities need to be developed and put in place to protect innocent livelihoods, and even lives, from the dramatic harm that can occur via weapons deployed at scale with no little to no variable cost.\nIndividual education on how to stay safe in the rising digital world will also continue to be incredibly important. Simple things like password complexity and reuse will continue to remain integral, but so will growing the capability for detecting potential fraud situations and separating the wheat from the chaff in situations where information is unverified and has the potential to be heavily biased.\nThe responsibility of keeping the world safe and secure does not just rest on the shoulders of individuals. It rests more squarely on the shoulders of companies and governments that develop the products, platforms, and services upon which humanity stands and runs their lives. These platforms need to be secure to a fault and need to be carefully watched over. A secure system needs to be designed from the beginning. Security is not an afterthought. It\u0026rsquo;s not a line item. It\u0026rsquo;s a core mentality and a foundation.\nFind the Book Here: Tools and Weapons by Brad Smith\n","permalink":"https://tylerwebb.io/posts/tools_and_weapons_part_2_cybersecurity-copy/","summary":"\u003ch1 id=\"cyber-security-the-wake-up-call-for-the-world\"\u003eCyber Security: The Wake-up Call for the World\u003c/h1\u003e\n\u003chr\u003e\n\u003cp\u003eHumans love shortcuts. We love finding better, easier, and quicker ways of doing things. This has allowed us to innovate and automate away many of the mundane parts of life. It also has allowed us to progress the general standard of living exponentially.\u003c/p\u003e\n\u003cp\u003eThis shortcut nature usually proves us well. We\u0026rsquo;re scrappy, we\u0026rsquo;re creative, and we know how to handle hard things. However, a critical ingredient to healthy progression is ethical direction. When Ethics are lacking, individuals and groups tend to look for shortcuts that bypass important issues such as human rights and ownership.\u003c/p\u003e","title":"Tools and Weapons Review Part 2: Cybersecurity - The Wake-Up Call for the World"},{"content":"Folding at Home and Rosetta at Home are both distributed computing platforms organized and put together to provide researchers with an alternative high-compute platform. These services take advantage of crowdsourcing and dormant computing power ranging from full server environments to last decade\u0026rsquo;s netbooks.\nBoth services work to compute the complex and diverse ways that proteins assemble themselves by folding. Proteins assume a particular shape in order to perform their various functions and interface with each other. By understanding more about the ways that proteins fold, the medical community can better understand the cause and effect of different proteins and develop better drugs and treatments for all kinds of diseases.\nThis is where raw compute power comes in. Since proteins are so complex and can fold in a potentially baffling amount of ways, possible folds need to be simulated in order to construct accurate models. This requires a lot of processing power and is commonly run on research supercomputers at University\u0026rsquo;s and Labs around the world. However, the cost of these supercomputers is high and the demand on their time is constant and commonly insufficient for the queue of work that needs to be run.\nFolding at Home, Rosetta, and many other similar projects have sprung up to help alleviate this constraint by asking people all over the world to donate their computers processing time to help perform these tasks at scale. It\u0026rsquo;s kind of like a DDoS attack against disease.\nFolding at Home Folding at Home is based out of the St. Louis School of Medicine at Washington University and is backed by labs and organizations from around the world.\nMain Website: Folding@Home\nTwitter: Folding@Home\nTwitch Steam: Folding@Home\nRosetta at Home Rosetta at Home is based out of the Baker Laboratory at the University of Washington and is distributed by the Berkeley Open Infrastructure for Network Computing Platform (BOINC) which hosts a number of other similar projects.\nMain Website: Rosetta@Home\nTwitter: Rosetta@Home\nReaction to COVID-19 In the wake of the COVID-19 pandemic, both of these platforms have exploded both in user base and computing power, with Folding at Home surpassing the exaflop barrier in a matter of weeks.\nIn fact, the initial reaction was so large that both of these services initially had trouble providing enough work to the community and receiving the results from this work, both relatively good problems to have. The teams behind Folding at Home and Rosetta, along with a numerous host of partners and donors, got to work and solved problem after problem at break-neck speed as they strove to take advantage of the new windfall of support and raw computing power.\nBeginning in early March of 2020, Folding at Home began to experience a tremendous spike in its user base. In February of 2020 Folding at Home had an active user base of ~30,000. By the end of March and beginning of April they had blown past the 400,000 user mark. Much of this initial growth was spurred on by the tech enthusiast community lead by groups like Nvidia, EVGA, and Linus Tech Tips.\nThis exponential increase in computing power put both these teams on their toes and there were a few days around March 20th when many machines were left on idle as infrastructure was quickly deployed. Since then both services have been incredibly busy partnering and working with outside groups to utilize what have effectively become some of the most powerful compute platforms in the world.\nSource: Twitter\nOn March 25th, 2020 Folding at Home broke the exaFlop barrier! Source: Twitter\nSource: Twitter\nOn March 30th, 2020 Folding at Home broke the 1 Million device mark! Source: Twitter\nIn these curious times, truly remarkable people have stepped up to accomplish extraordinary things.\nMy Cluster I\u0026rsquo;ve had the opportunity to participate in these events first hand as a Folding at Home and Rosetta at Home donor. I currently run two desktop towers and an older Mac Mini full time contributing to these two platforms. I have all of these devices setup via SSH and VNC so that I can check in every once in a while to see how they\u0026rsquo;re doing. It has also served as a great way to heat the room :)\nSource: The Webb Cluster\nGet Involved Get involved with Folding@Home or Rosetta@Home by following the links below:\nFolding@Home\nRosetta@Home\n","permalink":"https://tylerwebb.io/posts/folding_at_home/folding_at_home/","summary":"\u003cp\u003eFolding at Home and Rosetta at Home are both distributed computing platforms organized and put together to provide researchers with an alternative high-compute platform. These services take advantage of crowdsourcing and dormant computing power ranging from full server environments to last decade\u0026rsquo;s netbooks.\u003c/p\u003e\n\u003cp\u003eBoth services work to compute the complex and diverse ways that proteins assemble themselves by folding. Proteins assume a particular shape in order to perform their various functions and interface with each other. By understanding more about the ways that proteins fold, the medical community can better understand the cause and effect of different proteins and develop better drugs and treatments for all kinds of diseases.\u003c/p\u003e","title":"Folding at Home and Rosetta"},{"content":"Check out the Repo and Readme on GitHub: Ceres on Github\nI built Ceres as part of my time with the Campus Web Services team at the University of Arizona to monitor our vast portfolio of sites. Ceres pays attention to the site connection over HTTP and if it detects a problem it will log the potential outage in a DynamoDB table and notify a specified Slack Channel. Once it detects the end of the potential outage it will update the database entry and notify the channel again.\nCeres is built to be deployed using Terraform on Amazon Web Services and coded primarily in Python.\nPull requests welcome :)\n","permalink":"https://tylerwebb.io/posts/ceres/","summary":"\u003cp\u003eCheck out the Repo and Readme on GitHub: \u003cbutton style=\"background:gray\"\u003e\u003ca href=\"https://github.com/uaz-web/ceres\"\u003eCeres on Github\u003c/a\u003e\u003c/button\u003e\u003c/p\u003e\n\u003cp\u003eI built Ceres as part of my time with the Campus Web Services team at the University of Arizona to monitor our vast portfolio of sites. Ceres pays attention to the site connection over HTTP and if it detects a problem it will log the potential outage in a DynamoDB table and notify a specified Slack Channel. Once it detects the end of the potential outage it will update the database entry and notify the channel again.\u003c/p\u003e","title":"Monitoring Websites with Ceres"},{"content":"Within Tools and Weapons is a treasure trove of knowledge about how to appropriately balance the exhilarating speed, agility, and enabling power of technical innovation with the important aspects of protecting human rights and working towards the benefit of society.\nBrad Smith comes from a background as a lawyer and general counsel of Microsoft where he has helped navigate many complex legal and ethical issues for the company and the tech sector as a whole. He has had a front row seat to complex world issues and has met and worked with leaders the world over. He has also worked hard to stay connected to the vast and unique users of not only the technology supplied by Microsoft but groups worldwide. This post is part of a series where I give some of my thoughts and takeaways from Tools and Weapons.\nPrivacy: A Fundamental Human Right Stories have always been a part of humanity. They have been used to entertain, convey information, and pass down history. These stories have taken many forms over time. They\u0026rsquo;ve been conveyed orally, written page by page, printed in mass, and recently been digitized for broad distribution.\nThe stories of humanity, the stories and intricate details of individuals, have gotten tied up in the massive amount of data collection that is taking place as part of the digital age. This data is stored in highly redundant ways, designed to last far into the future, in many cases long after the life of the individual or group who generated it. The technology behind this data storage is designed with high levels of integrity to avoid accidental or mechanical failures.\nData is organized and becomes information. Information is analyzed and becomes knowledge. Knowledge then enables informed action. This transformation process has become integral in the information age. We\u0026rsquo;ve increasingly become better and better at transforming data into information, sometimes to a fault. The modern datacenter has been an important piece in allowing the mass collection of data. The question has changed from how to effectively store data at scale, to how to utilize all the data points pouring in.\nWith the ability to collect and store the tome of human kind, the question of who owns what becomes integral to not just the ownership but the applied use of the data collected. While many companies have worked to remedy this question through the use of long and complicated End User License Agreements (EULAs) that are tapped through daily without a moment\u0026rsquo;s hesitation, there still exists a deeply complicated question of ownership.\nData is no respecter of country boundaries. It has the ability to exist in many places at one time and can potentially be accessed from anywhere in the world (barring geo-restrictions and \u0026ldquo;Great Firewalls\u0026rdquo;). This has proved to be uncharted territory for many governments, especially in the world of criminal investigations which can attempt to require companies to turn over data on foriegn customers. Tech companies often exist as an international entity and have to juggle lawful requests for information access with the precedent such actions can set for their international customers and for other governments.\nBrad Smith grapples with these problems throughout his book as he discusses many aspects of the concept of privacy and the complicated relationship of data collection between organizations and individuals and who actually has control of what data and to what degree. This is not a question that has a simple question and it will require lots of specific answers to very specific use cases.\nSince data has global influence and persistence, so does the influence that individual countries have on the use of data. A great example of this is the passing of the General Data Protection Regulation by the European Union in 2018. The GDPR provides certain rights to the original creators of data which include customers rights to know what data a company has on them, the rights to change inaccurate data, and the right to delete their data (based on circumstance). While these protections are granted only to members of the European Union, they have created large waves worldwide. Many companies have chosen to adopt the regulations and guidelines not just for their European customers but for all of their customers.\nWhile the GDPR is an example of data protections for individuals, what happens when another country decides to have a differing view on data use? Suppose a country decides to put into place legislation that gives companies full ownership over the data they collect. How will this affect companies as they attempt to juggle different regulations for different customers across a variety of nations that are becoming more digitally connected every day.\nThese questions transcend the policies or practices of a singular country. Governments will need to come together to determine what and how regulation will be put into place for data and a variety of other global issues.\n”How can governments regulate a technology that is bigger than themselves? This is perhaps the single greatest conundrum confronting technology\u0026rsquo;s regulatory future. But once you ask the question, one part of the answer becomes clear: Governments will need to work together.” - Tools and Weapons pg. 300\nInternational agreements and ventures will be critical in the coming years to provide clarity and direction on subjects of not just privacy, but other critical details regarding the digital world.\nFind the Book Here: Tools and Weapons by Brad Smith\n","permalink":"https://tylerwebb.io/posts/tools_and_weapons_part_1_privacy/","summary":"\u003cp\u003eWithin \u003cem\u003eTools and Weapons\u003c/em\u003e is a treasure trove of knowledge about how to appropriately balance the exhilarating speed, agility, and enabling power of technical innovation with the important aspects of protecting human rights and working towards the benefit of society.\u003c/p\u003e\n\u003cp\u003eBrad Smith comes from a background as a lawyer and general counsel of Microsoft where he has helped navigate many complex legal and ethical issues for the company and the tech sector as a whole. He has had a front row seat to complex world issues and has met and worked with leaders the world over. He has also worked hard to stay connected to the vast and unique users of not only the technology supplied by Microsoft but groups worldwide. This post is part of a series where I give some of my thoughts and takeaways from \u003cem\u003eTools and Weapons\u003c/em\u003e.\u003c/p\u003e","title":"Tools and Weapons Review Part 1: A Discussion about Privacy"},{"content":"KUBERNETES: In the quest to further abstract the technology stack and help developers deploy and scale their applications better than ever, I present to you Kubernetes. Kubernetes is a leader in the technical segment of container orchestration and is designed to be run on many types of platforms and runtimes. As one of the top contributed to projects on GitHub, Kubernetes is an open-source wonder having spun off from the Borg project at Google. Kubernetes is written in Golang and is used by companies such as Capital One, eBay, and Spotify.\nA key ingredient to the way that Kubernetes handles loads and scaling is the concept of containerization of environments. Containers are a clever way to bundle together the software and dependencies required to run a service repeatedly and reliability. If configured correctly, containers also lend themselves well to auto-scaling. This allows for repeated deployment without having to worry about changes to the runtime or changes to the code base or dependencies. Containers can be hosted in repositories and pulled down when needed.\nContainers oftentimes get confused with the concept of a virtual machine. The key difference between these two virtualization technologies is the concept of a shared kernel. Where virtual machines are deployed having separate user spaces and separate kernels, containers share kernels while keeping their processes and user spaces separate.\nA very popular containerization tool is called Docker. Docker bundles together the code required for building and running applications and creates an image which can be deployed in the same state repeatably. Docker is the containerization tool that Kubernetes most often takes advantage of.\nKubernetes takes advantage of these characteristics of containers in order to perform tasks at scale. The architecture of Kubernetes is broken down into a few key pieces which make up a Kubernetes Cluster.\nSource: Nodes and Pods Image\nPods: A pod is a grouping of one or more containers that share storage and network resources. Each pod also has a specification about how to run the containers it controls. These containers within a pod are able to find each other via a localhost connection. In order to directly talk to other containers in different pods, the configuration needs to be setup specifically to allow this. In many ways a pod is similar to an abstracted version of a machine host.\nPods allow easier management and simplify deployment. One pod deploys multiple resources and can be deployed repeatably to accomplish the same or similar tasks at scale. Pods take advantage of many of the underlying advantages of infrastructure as templates.\nNodes: A node is also referred to as a minion (or sometimes worker). This is the machine that the containers and pods are deployed on top of. The node needs to have a runtime installed such as Docker. Each node is managed by the Kubernetes master and each node can contain multiple pods.\nTwo important parts of the node are the Kubelet and the Container runtime. The Kubelet takes responsibility for communicating between the Kubernetes master and the node. The runtime pulls down the container repositories from an image registry (like Docker Hub), unpacks the container, and runs the application.\nKubernetes Master: The Kubernetes Master is a specialized node which takes responsibility for the overall coordination of the cluster. It runs three main processes. These are the kube-apiserver, the kube-controller-manager, and the kube-scheduler. The kube-apiserver handles the REST operations for the cluster. The kube-controller-manager keeps a virtual eye on the cluster’s current state and takes actions to bring it the desired state if things get off track. The kube-scheduler has a large impact on the availability, performance, and capacity of the cluster as it accounts for the current state and schedules changes accordingly.\nTakeaways: Each of these components of a Kubernetes cluster is critical to the success of the container orchestration. Containers can be spun up as demand increases, some level of resiliency can be had with multiple pods running on separate nodes.\nKubernetes may not be the tool of choice when it comes to running a singular process, but when you have a fully fledged application running on multiple languages, each with their own separate dependencies and runtime requirements, each referencing a series of databases and requiring specific permissions, a Kubernetes cluster is definitely an viable option to consider.\nLearn more from this 5 minute introduction to Kubernetes from VMWare:\n","permalink":"https://tylerwebb.io/posts/kubernetes_crash_course/kubernetes_crash_course/","summary":"\u003ch2 id=\"kubernetes\"\u003eKUBERNETES:\u003c/h2\u003e\n\u003chr\u003e\n\u003cp\u003eIn the quest to further abstract the technology stack and help developers deploy and scale their applications better than ever, I present to you \u003ca href=\"https://kubernetes.io/\"\u003eKubernetes\u003c/a\u003e. Kubernetes is a leader in the technical segment of container orchestration and is designed to be run on many types of platforms and runtimes. As one of the top contributed to projects on GitHub, Kubernetes is an open-source wonder having spun off from the Borg project at Google. Kubernetes is written in \u003ca href=\"https://golang.org/\"\u003eGolang\u003c/a\u003e and is used by companies such as Capital One, eBay, and Spotify.\u003c/p\u003e","title":"Kubernetes: Containerization on Steriods"},{"content":"The Unicorn Project is written as a quasi-sequel to the widely popular book, The Phoenix Project. This book is written in the third person (whereas the Phoenix Project is written in the first-person point of view of Bill Palmer) and follows the adventures of Maxine, a rockstar developer with years of experience pushing critical code into production. Maxine works on a team that manages some critical control systems for Parts Unlimited, a major auto part manufacturer. However, when a critical system goes down she is left holding the bag and is exiled to the all so dreaded Phoenix Project, an initiative designed to bring Parts Unlimited into the eCommerce age. She arrives there right as Bill Palmer shows up to the scene as well.\nMaxine quickly realizes that the Phoenix Project, a multi-team initiative, is far worse than she imagined. Every part of the development process is gridlocked by ticketing systems, non-responsive gatekeepers, and broken build environments. She struggles to even get a build environment running on her local machine. That is until she encounters the Resistance.\nThe Resistance formed out of the need to work outside the gridlock to push forward the critical work of the company. Maxine gets involved and the team quickly begins working on fixing the broken processes and systems currently hindering effective and efficient development. They are helped along by a mysterious individual named Erik (yep, the same guy from the Phoenix Project book who helps Bill Palmer). Erik teaches the Resistance the Five Ideals of Development. These are:\nThe Five Ideals The First Ideal: Locality and Simplicity The Second Ideal: Focus, Flow, and Joy The Third Ideal: Improvement of Daily Work The Fourth Ideal: Psychological Safety The Fifth Ideal: Customer Focus When these five ideals are correctly focused on in the developer community, development begins to happen at a rapid pace that encourages course correction and providing the absolute best value for the customer. This, in turn, drives growth forward which pays dividends for years to come for a business.\nThe Unicorn Project also covers the three horizons of growth. The horizons are the pipeline through which new ideas and innovation flows for a company. The Unicorn Project notes that if you\u0026rsquo;re not growing, you\u0026rsquo;re declining. New ideas enter a company in the Horizon 3 category. They are prototyped, tested, and evaluated. These are oftentimes referred to as the question mark category. If there is potential for continued growth they graduate to the Horizon 2 category. They are further tested and implemented in the company under a more official status. Finally, these lines enter Horizon 1. These are considered cash cows for a company where market share has been conquered and the fight has been won. However, as with all good ideas turned steady income, Horizon 1\u0026rsquo;s will eventually move into the decline phase. At this point, the company needs to make sure they have solid Horizon 2 and 3 projects in the pipeline to eventually fill in the roles of Horizon 1 to continue to stimulate growth.\nThis book is a must-read for anyone who wants to more fully understand the way to sustained growth via effective software development process.\nFind the book here: The Unicorn Project\n","permalink":"https://tylerwebb.io/posts/unicorn_project/","summary":"\u003cp\u003eThe Unicorn Project is written as a quasi-sequel to the widely popular book, The Phoenix Project. This book is written in the third person (whereas the Phoenix Project is written in the first-person point of view of Bill Palmer) and follows the adventures of Maxine, a rockstar developer with years of experience pushing critical code into production. Maxine works on a team that manages some critical control systems for Parts Unlimited, a major auto part manufacturer. However, when a critical system goes down she is left holding the bag and is exiled to the all so dreaded Phoenix Project, an initiative designed to bring Parts Unlimited into the eCommerce age. She arrives there right as Bill Palmer shows up to the scene as well.\u003c/p\u003e","title":"Review of The Unicorn Project: A Novel about Developers, Digital Distribution, and Thriving in the Digital Age of Data"},{"content":"I recently finished reading “The Phoenix Project” written by Gene Kim, Kevin Bahr, and George Spafford. This book is a must read for anyone trying to wrap their heads around the buzzword DevOps.\nThe story follows a man named Bill who works for a company called Parts Unlimited (basically just insert any generic Fortune 500 here). He manages a department in his company\u0026rsquo;s IT apparatus and likes what he does. One day, without warning, he is unexpectedly promoted to Vice President of IT Operations. Long story short, Parts Unlimited is having some serious financial problems and the future success of the company rests on the rollout of a special initiative to establish an effective e-commerce presence, dubbed The Phoenix Project. Bill is tasked with the successful rollout of Phoenix, as well as putting out almost daily, technical fires.\nAlong the way Bill meets Erik (a mysterious member of the board) who mentors him on the right way to manage the development and operations processes of IT.\nAlong the way Erik enlightens Bill with the 3 ways. These are:\nThe Principles of Flow Making work visible Limiting work in progress (WIP) Reducing batch sizes and the number of handoffs Continuous identification and evaluation of constraints Eliminating hardships of daily work The Principles of Feedback Seeing Problems as they occur Swarming and solving problems with new knowledge Pushing quality closer to the source Continually optimizing for downstream work centers The Principles of Continual Learning and Experimentation Enabling high trust and Accepting that failures will always occur when a system is complex Making it acceptable to talk about problems to encourage a safe system of work Making the improvement of daily work part of your institution Converting local learnings into global learning to be used by the entire organization The third way is tied in fundamentally to the first two ways When these three ways are put in practice, you are well on your way to having successfully implemented DevOps into your company or organization. This will allow for more effective use of limited resources, higher throughput, and better quality of product from the very beginning.\nThe Principles of Flow help to streamline processses and remove the extra fluff that can build up and slow down the workflow. The Principles of Feedback work to provide valuable feedback on how the principles of flow are working and can catch problems quickly and efficiently. The Principles of Continual Learning and Experimentation help to implement the feedback recieved into the current and future processes and refine them.\nWhile DevOps is sometimes seen as one of those technology buzzwords, it is a practice that provides value to any company or organization that works with processes and production (both physical and digital).\nFind the book here: The Phoenix Project\n","permalink":"https://tylerwebb.io/posts/phoenix_project/","summary":"\u003cp\u003eI recently finished reading \u003cem\u003e“The Phoenix Project”\u003c/em\u003e written by Gene Kim, Kevin Bahr, and George Spafford. This book is a must read for anyone trying to wrap their heads around the buzzword \u003cem\u003eDevOps\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe story follows a man named Bill who works for a company called Parts Unlimited (basically just insert any generic Fortune 500 here). He manages a department in his company\u0026rsquo;s IT apparatus and likes what he does. One day, without warning, he is unexpectedly promoted to Vice President of IT Operations. Long story short, Parts Unlimited is having some serious financial problems and the future success of the company rests on the rollout of a special initiative to establish an effective e-commerce presence, dubbed The Phoenix Project. Bill is tasked with the successful rollout of Phoenix, as well as putting out almost daily, technical fires.\u003c/p\u003e","title":"Review of The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win "}]